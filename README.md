sirene-dataquality-monitor

Projet de monitoring de qualitÃ© de donnÃ©es basÃ© sur les donnÃ©es publiques SIRENE (INSEE), limitÃ© aux Ã©tablissements du dÃ©partement 44 (Loire-Atlantique).

Le projet met en place un pipeline data reproductible dans Supabase (PostgreSQL) avec :

ingestion de donnÃ©es CSV volumineuses (â‰ˆ 8,8 Go)

normalisation via vues SQL

rÃ¨gles de Data Quality formalisÃ©es

historisation des imports

suivi qualitÃ© par import

visualisation interactive via Streamlit

Stack 100 % gratuite, orientÃ©e Data Engineering / Analytics.

ğŸ¯ Objectifs

Importer les donnÃ©es SIRENE (INSEE)

Filtrer le pÃ©rimÃ¨tre dÃ©partement 44

Charger les donnÃ©es dans PostgreSQL (Supabase)

Mettre en place un monitoring qualitÃ© structurÃ©

Suivre la qualitÃ© dans le temps

Exposer des rÃ©sultats lisibles pour analyse et dÃ©monstration

ğŸ—‚ï¸ Structure du projet
sirene-dataquality-monitor/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                      # DonnÃ©es brutes INSEE (non versionnÃ©es)
â”‚   â””â”€â”€ processed/                # CSV filtrÃ© sirene_44.csv (non versionnÃ©)
â”œâ”€â”€ ingest/
â”‚   â”œâ”€â”€ filter_sirene_44.py       # Filtrage dÃ©partement 44
â”‚   â””â”€â”€ load_to_supabase.sh       # Import PostgreSQL (TRUNCATE + COPY)
â”œâ”€â”€ sql/
â”‚   â”œâ”€â”€ 001_view_v_sirene_44.sql
â”‚   â”œâ”€â”€ 010_dq_checks.sql
â”‚   â”œâ”€â”€ 020_view_v_sirene_44_analytics.sql
â”‚   â”œâ”€â”€ 030_create_dq_results.sql
â”‚   â”œâ”€â”€ 031_run_dq_rules.sql
â”‚   â”œâ”€â”€ 040_create_import_runs.sql
â”‚   â””â”€â”€ 050_view_v_dq_by_import.sql
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ run_pipeline.sh           # Orchestration complÃ¨te du pipeline
â”œâ”€â”€ streamlit_app/
â”‚   â”œâ”€â”€ app.py                    # Application Streamlit
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ HOW_TO_DEV.md                 # Runbook dÃ©veloppeur (non public)
â”œâ”€â”€ .env.example
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md

âš™ï¸ PrÃ©requis

Python 3.10+

Client PostgreSQL (psql)

AccÃ¨s Ã  une base PostgreSQL (Supabase)

Environnement Linux / WSL recommandÃ©

ğŸ” Variables dâ€™environnement

CrÃ©er un fichier .env (non versionnÃ©) Ã  partir de .env.example :

DATABASE_URL=postgresql://<ADMIN_USER>:<PASSWORD>@<HOST>:5432/postgres
SIRENE_44_CSV=/chemin/absolu/data/processed/sirene_44.csv


Charger les variables :

set -a; source .env; set +a

ğŸ“¥ DonnÃ©es source

Jeu : SIRENE â€“ StockEtablissement

Source : INSEE / data.gouv.fr

Format : CSV UTF-8

Taille : ~8,8 Go dÃ©compressÃ©

Fichier attendu :

data/raw/StockEtablissement_utf8.csv


âš ï¸ Les donnÃ©es ne sont pas versionnÃ©es (voir .gitignore).

ğŸ” Filtrage dÃ©partement 44

Script :

ingest/filter_sirene_44.py


Fonctionnement :

lecture du CSV SIRENE complet

filtrage sur codePostalEtablissement LIKE '44%'

Ã©criture dâ€™un CSV rÃ©duit

Sortie :

data/processed/sirene_44.csv


â‰ˆ 676 000 lignes

ğŸ—„ï¸ Base de donnÃ©es

PostgreSQL hÃ©bergÃ© sur Supabase

Table brute : sirene_44

Colonnes en TEXT (schÃ©ma identique au CSV)

Import via COPY PostgreSQL (robuste sur gros volumes)

ğŸ§¼ Vue clean

Vue : v_sirene_44

Objectifs :

noms de colonnes en snake_case

chaÃ®nes vides converties en NULL

SQL sans guillemets

base stable pour transformations

DÃ©finition :

sql/001_view_v_sirene_44.sql

ğŸ“Š Vue analytics

Vue : v_sirene_44_analytics

Ajouts :

typage logique

indicateurs dÃ©rivÃ©s

flags qualitÃ© (validitÃ© SIRET, statut actifâ€¦)

Base prÃªte pour :

rÃ¨gles Data Quality

analyses mÃ©tier

dashboards

DÃ©finition :

sql/020_view_v_sirene_44_analytics.sql

ğŸ§ª Data Quality
Table de rÃ©sultats

Table : dq_results

Contient :

code de rÃ¨gle

libellÃ©

mÃ©trique calculÃ©e

seuil

statut (OK / KO)

timestamp dâ€™exÃ©cution

RÃ¨gles implÃ©mentÃ©es

ACTIVE_RATE_RECENT
Taux dâ€™Ã©tablissements actifs (crÃ©Ã©s aprÃ¨s 2010) â‰¥ 50 %

CP_NULL_RATE
Taux de codes postaux NULL < 0,5 %

SIRET_INVALID_RATE
Taux de SIRET invalides < 1 %

DÃ©finition et exÃ©cution :

sql/031_run_dq_rules.sql

ğŸ•’ Historisation des imports

Table : sirene_import_runs

Chaque exÃ©cution du pipeline enregistre :

date dâ€™import

fichier source

nombre de lignes

identifiant dâ€™import (import_id)

ğŸ“ˆ Vue Data Quality par import

Vue : v_dq_by_import

Fonction :

associe chaque import Ã  sa derniÃ¨re exÃ©cution DQ

une ligne par rÃ¨gle et par import

base unique pour BI et Streamlit

DÃ©finition :

sql/050_view_v_dq_by_import.sql

â–¶ï¸ ExÃ©cution du pipeline complet

Un script unique permet de rejouer tout le pipeline :

bash scripts/run_pipeline.sh


Ã‰tapes incluses :

Filtrage CSV

Import Supabase (TRUNCATE + COPY)

CrÃ©ation / mise Ã  jour des vues

ExÃ©cution des rÃ¨gles Data Quality

Historisation de lâ€™import

ğŸ“Š Application Streamlit

Lâ€™application Streamlit consomme exclusivement des vues SQL.

FonctionnalitÃ©s :

sÃ©lection dâ€™un import

statut global OK / KO

dÃ©tail par rÃ¨gle

filtre â€œKO uniquementâ€

Lancement local :

streamlit run streamlit_app/app.py

ğŸ” SÃ©curitÃ©

RÃ´le PostgreSQL dq_readonly

AccÃ¨s lecture seule

Aucune table brute exposÃ©e

Connexion via Session Pooler Supabase (IPv4 compatible)

UtilisÃ© pour Streamlit et accÃ¨s public

ğŸ“– Documentation dÃ©veloppeur

Un runbook interne est disponible :

HOW_TO_DEV.md


Il dÃ©crit :

setup local

gestion des rÃ´les PostgreSQL

exÃ©cution pas Ã  pas

points de vigilance / debug

â¡ï¸ Ce fichier nâ€™est pas destinÃ© au public
â¡ï¸ Il peut rester versionnÃ© ou non selon ton choix

ğŸ§  Choix techniques

PostgreSQL (Supabase) : gratuit, robuste, SQL natif

COPY PostgreSQL : performant sur gros volumes

Table brute + vues : sÃ©paration ingestion / logique mÃ©tier

Monitoring SQL versionnÃ© : explicite, traÃ§able

Streamlit : simple, rapide, dÃ©monstratif

