# sirene-dataquality-monitor
sirene-dataquality-monitor

# Sirene Data Quality Monitor

Ce projet permet d'extraire, filtrer et charger les donnÃ©es **SIRENE â€“ Ã©tablissements du dÃ©partement 44 (Loire-Atlantique)** dans une base **Supabase PostgreSQL**, de maniÃ¨re **reproductible, fiable et documentÃ©e**.

---

## ğŸ¯ Objectif

* TÃ©lÃ©charger les donnÃ©es SIRENE officielles (INSEE)
* Filtrer uniquement le **dÃ©partement 44**
* Charger les donnÃ©es dans **Supabase PostgreSQL**
* Disposer d'une base propre pour analyses, data quality, dashboards

---

## ğŸ—‚ï¸ Structure du projet

```
sirene-dataquality-monitor/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/            # DonnÃ©es brutes (CSV volumineux, ignorÃ© par git)
â”‚   â””â”€â”€ processed/      # DonnÃ©es filtrÃ©es (sirene_44.csv)
â”œâ”€â”€ ingest/
â”‚   â”œâ”€â”€ filter_sirene_44.py     # Filtre dÃ©partement 44
â”‚   â”œâ”€â”€ import_sirene_44.py     # (optionnel) import via Python
â”‚   â””â”€â”€ load_to_supabase.sh     # Import PostgreSQL via COPY (mÃ©thode retenue)
â”œâ”€â”€ .env.example
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```

---

## ğŸ“¥ 1. DonnÃ©es source (INSEE)

TÃ©lÃ©charger depuis data.gouv.fr :

* **Jeu** : *Sirene â€“ Fichier StockEtablissement*
* **Format** : CSV (UTF-8)
* **Fichier final** :

```
StockEtablissement_utf8.csv  (~8,8 Go)
```

Placer le fichier ici :

```
data/raw/StockEtablissement_utf8.csv
```

âš ï¸ Ce fichier est **volumineux** et doit rester hors Git (`.gitignore`).

---

## ğŸ” 2. Filtrage dÃ©partement 44

Script :

```
ingest/filter_sirene_44.py
```

Lancer :

```bash
python ingest/filter_sirene_44.py
```

RÃ©sultat :

```
data/processed/sirene_44.csv
```

Environ **676 000 lignes**.

---

## ğŸ” 3. Configuration Supabase

### CrÃ©er un projet Supabase

* RÃ©gion par dÃ©faut
* Plan gratuit

### RÃ©cupÃ©rer la connexion PostgreSQL

Dans Supabase â†’ **Connect** â†’ Connection String :

* Type : `URI`
* Method : **Direct connection**

âš ï¸ Attention : IPv6 par dÃ©faut. Fonctionne sous WSL **aprÃ¨s configuration DNS**.

---

## ğŸ”§ 4. Variables d'environnement

CrÃ©er le fichier `.env` (non versionnÃ©) :

```bash
cp .env.example .env
```

Exemple `.env` :

```env
DATABASE_URL=postgresql://postgres:PASSWORD@db.xxxxx.supabase.co:5432/postgres
SIRENE_44_CSV=/home/USER/Repos/sirene-dataquality-monitor/data/processed/sirene_44.csv
```

Charger les variables :

```bash
set -a; source .env; set +a
```

Tester la connexion :

```bash
psql "$DATABASE_URL" -c "select now();"
```

---

## ğŸ—ï¸ 5. CrÃ©ation automatique de la table

âš ï¸ Ã‰tape **clÃ©** : la table doit correspondre **exactement** aux colonnes CSV.

Commande Ã  exÃ©cuter **telle quelle** :

```bash
psql "$DATABASE_URL" -c "$(python - <<'PY'
import pandas as pd
cols = pd.read_csv('data/processed/sirene_44.csv', nrows=1).columns.tolist()
sql = 'DROP TABLE IF EXISTS public.sirene_44; CREATE TABLE public.sirene_44 ('
sql += ','.join([f'\"{c}\" text' for c in cols])
sql += ');'
print(sql)
PY
)"
```

Cette commande :

* lit les colonnes du CSV
* supprime la table si elle existe
* recrÃ©e la table avec le bon schÃ©ma

---

## ğŸ“¤ 6. Import des donnÃ©es dans Supabase

Script utilisÃ© (mÃ©thode retenue) :

```
ingest/load_to_supabase.sh
```

Rendre exÃ©cutable :

```bash
chmod +x ingest/load_to_supabase.sh
```

Lancer l'import :

```bash
bash ingest/load_to_supabase.sh
```

Sortie attendue :

```
âœ‚ï¸ Truncate table
ğŸ“¥ Import CSV
COPY 676473
âœ… VÃ©rification
```

---

## âœ… 7. VÃ©rification finale

```bash
psql "$DATABASE_URL" -c "SELECT COUNT(*) FROM public.sirene_44;"
```

RÃ©sultat attendu :

```
676473
```

---

## ğŸ§  Choix techniques

* **COPY PostgreSQL** prÃ©fÃ©rÃ© Ã  pandas/ORM :

  * plus rapide
  * plus fiable sur gros volumes
  * reproductible
* Table en `TEXT` volontairement :

  * pas de blocage Ã  l'import
  * typage possible ultÃ©rieurement

---

## ğŸ” Rejouer l'import

Ã€ tout moment :

```bash
bash ingest/load_to_supabase.sh
```

La table est **vidÃ©e puis rechargÃ©e** (mode `TRUNCATE`).

---

## ğŸ“Œ Prochaines Ã©tapes possibles

* Index (codePostal, siret)
* Data quality checks
* Dashboards (Metabase / Streamlit)
* Historisation

---

âœ… **Pipeline validÃ© et reproductible**


1. filter_sirene_44.py   â†’ crÃ©e un CSV filtrÃ© (44 uniquement)
2. import_sirene_44.py   â†’ charge ce CSV dans Supabase

## Pipeline d'ingestion

1. TÃ©lÃ©charger le fichier SIRENE StockEtablissement (CSV UTF-8)
2. Le placer dans `data/raw/`
3. Filtrer le dÃ©partement 44 :
   ```bash
   python ingest/filter_sirene_44.py

##########################################

# Sirene Data Quality Monitor (DÃ©pt 44)

Objectif : tÃ©lÃ©charger le fichier SIRENE â€œStockEtablissementâ€, filtrer les Ã©tablissements du **dÃ©partement 44** puis importer le rÃ©sultat dans **Supabase Postgres** pour analyses/qualitÃ©.

---

## 0) PrÃ©-requis

- Linux / WSL (Ubuntu) + Python 3
- `psql` installÃ© (client PostgreSQL)

### Installer psql (Ubuntu/WSL)
```bash
sudo apt update
sudo apt install -y postgresql-client
psql --version
