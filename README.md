# sirene-dataquality-monitor

Projet de **monitoring de la qualitÃ© des donnÃ©es** basÃ© sur les donnÃ©es publiques **SIRENE (INSEE)**, limitÃ© aux Ã©tablissements du **dÃ©partement 44 â€“ Loire-Atlantique**.

Lâ€™objectif est de mettre en place un **pipeline data reproductible**, 100â€¯% gratuit, orientÃ© **Data Engineering / Analytics**, avec stockage PostgreSQL (Supabase) et visualisation via Streamlit.

---

## âœ¨ FonctionnalitÃ©s

- Ingestion de **CSV volumineux** (~8,8 Go)
- Filtrage gÃ©ographique (dÃ©partement 44)
- Chargement robuste via **COPY PostgreSQL**
- Normalisation des donnÃ©es via **vues SQL**
- RÃ¨gles de **Data Quality** formalisÃ©es et versionnÃ©es
- Historisation des imports
- Suivi de la qualitÃ© **dans le temps**
- Visualisation interactive via **Streamlit**

---

## ğŸ¯ Objectifs

- Importer les donnÃ©es SIRENE (INSEE)
- Filtrer le pÃ©rimÃ¨tre au dÃ©partement 44
- Charger les donnÃ©es dans PostgreSQL (Supabase)
- Mettre en place un monitoring qualitÃ© structurÃ©
- Suivre la qualitÃ© des donnÃ©es dans le temps
- Exposer des rÃ©sultats lisibles pour analyse et dÃ©monstration

---

## ğŸ—‚ï¸ Structure du projet

```
sirene-dataquality-monitor/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                      # DonnÃ©es brutes INSEE (non versionnÃ©es)
â”‚   â””â”€â”€ processed/                # CSV filtrÃ© sirene_44.csv (non versionnÃ©)
â”œâ”€â”€ ingest/
â”‚   â”œâ”€â”€ filter_sirene_44.py       # Filtrage dÃ©partement 44
â”‚   â””â”€â”€ load_to_supabase.sh       # Import PostgreSQL (TRUNCATE + COPY)
â”œâ”€â”€ sql/
â”‚   â”œâ”€â”€ 001_view_v_sirene_44.sql
â”‚   â”œâ”€â”€ 010_dq_checks.sql
â”‚   â”œâ”€â”€ 020_view_v_sirene_44_analytics.sql
â”‚   â”œâ”€â”€ 030_create_dq_results.sql
â”‚   â”œâ”€â”€ 031_run_dq_rules.sql
â”‚   â”œâ”€â”€ 040_create_import_runs.sql
â”‚   â””â”€â”€ 050_view_v_dq_by_import.sql
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ run_pipeline.sh           # Orchestration complÃ¨te du pipeline
â”œâ”€â”€ streamlit_app/
â”‚   â”œâ”€â”€ app.py                    # Application Streamlit
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ HOW_TO_DEV.md                 # Runbook dÃ©veloppeur (non public)
â”œâ”€â”€ .env.example
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```

---

## âš™ï¸ PrÃ©requis

- Python **3.10+**
- Client PostgreSQL (`psql`)
- AccÃ¨s Ã  une base PostgreSQL (Supabase)
- Environnement **Linux / WSL recommandÃ©**

---

## ğŸ” Variables dâ€™environnement

CrÃ©er un fichier `.env` (non versionnÃ©) Ã  partir de `.env.example` :

```
DATABASE_URL=postgresql://<ADMIN_USER>:<PASSWORD>@<HOST>:5432/postgres
SIRENE_44_CSV=/chemin/absolu/data/processed/sirene_44.csv
```

Charger les variables :

```
set -a; source .env; set +a
```

---

## ğŸ“¥ DonnÃ©es source

- **Jeu** : SIRENE â€“ StockEtablissement
- **Source** : INSEE / data.gouv.fr
- **Format** : CSV UTF-8
- **Taille** : ~8,8 Go dÃ©compressÃ©

Fichier attendu :

```
data/raw/StockEtablissement_utf8.csv
```

âš ï¸ Les donnÃ©es ne sont **pas versionnÃ©es** (voir `.gitignore`).

---

## ğŸ” Filtrage dÃ©partement 44

**Script** :

```
ingest/filter_sirene_44.py
```

**Fonctionnement** :

- Lecture du CSV SIRENE complet
- Filtrage sur `codePostalEtablissement LIKE '44%'`
- Ã‰criture dâ€™un CSV rÃ©duit

**Sortie** :

```
data/processed/sirene_44.csv
```

â‰ˆ **676â€¯000 lignes**

---

## ğŸ—„ï¸ Base de donnÃ©es

- PostgreSQL hÃ©bergÃ© sur **Supabase**
- Table brute : `sirene_44`
- Colonnes en **TEXT** (schÃ©ma identique au CSV)
- Import via **COPY PostgreSQL** (robuste sur gros volumes)

---

## ğŸ§¼ Vue clean

**Vue** : `v_sirene_44`

Objectifs :

- Noms de colonnes en `snake_case`
- ChaÃ®nes vides converties en `NULL`
- SQL sans guillemets
- Base stable pour transformations

DÃ©finition :

```
sql/001_view_v_sirene_44.sql
```

---

## ğŸ“Š Vue analytics

**Vue** : `v_sirene_44_analytics`

Ajouts :

- Typage logique
- Indicateurs dÃ©rivÃ©s
- Flags qualitÃ© (validitÃ© SIRET, statut actifâ€¦)

Base prÃªte pour :

- RÃ¨gles Data Quality
- Analyses mÃ©tier
- Dashboards

DÃ©finition :

```
sql/020_view_v_sirene_44_analytics.sql
```

---

## ğŸ§ª Data Quality

### Table de rÃ©sultats

**Table** : `dq_results`

Contient :

- Code de rÃ¨gle
- LibellÃ©
- MÃ©trique calculÃ©e
- Seuil
- Statut (`OK` / `KO`)
- Timestamp dâ€™exÃ©cution

### RÃ¨gles implÃ©mentÃ©es

- **ACTIVE_RATE_RECENT**  
  Taux dâ€™Ã©tablissements actifs (crÃ©Ã©s aprÃ¨s 2010) â‰¥ 50â€¯%

- **CP_NULL_RATE**  
  Taux de codes postaux NULL < 0,5â€¯%

- **SIRET_INVALID_RATE**  
  Taux de SIRET invalides < 1â€¯%

DÃ©finition et exÃ©cution :

```
sql/031_run_dq_rules.sql
```

---

## ğŸ•’ Historisation des imports

**Table** : `sirene_import_runs`

Chaque exÃ©cution du pipeline enregistre :

- Date dâ€™import
- Fichier source
- Nombre de lignes
- Identifiant dâ€™import (`import_id`)

---

## ğŸ“ˆ Vue Data Quality par import

**Vue** : `v_dq_by_import`

Fonction :

- Associe chaque import Ã  sa derniÃ¨re exÃ©cution DQ
- Une ligne par rÃ¨gle et par import
- Base unique pour BI et Streamlit

DÃ©finition :

```
sql/050_view_v_dq_by_import.sql
```

---

## â–¶ï¸ ExÃ©cution du pipeline complet

Un script unique permet de rejouer tout le pipeline :

```
bash scripts/run_pipeline.sh
```

Ã‰tapes incluses :

- Filtrage CSV
- Import Supabase (`TRUNCATE + COPY`)
- CrÃ©ation / mise Ã  jour des vues
- ExÃ©cution des rÃ¨gles Data Quality
- Historisation de lâ€™import

---

## ğŸ“Š Application Streamlit

Lâ€™application Streamlit consomme **exclusivement des vues SQL**.

FonctionnalitÃ©s :

- SÃ©lection dâ€™un import
- Statut global `OK / KO`
- DÃ©tail par rÃ¨gle
- Filtre Â«â€¯KO uniquementâ€¯Â»

Lancement local :

```
streamlit run streamlit_app/app.py
```

---

## ğŸ” SÃ©curitÃ©

- RÃ´le PostgreSQL : `dq_readonly`
- AccÃ¨s **lecture seule**
- Aucune table brute exposÃ©e
- Connexion via **Session Pooler Supabase** (IPv4 compatible)
- UtilisÃ© pour Streamlit et accÃ¨s public

---

## ğŸ“– Documentation dÃ©veloppeur

Un runbook interne est disponible :

```
HOW_TO_DEV.md
```

Il dÃ©crit :

- Setup local
- Gestion des rÃ´les PostgreSQL
- ExÃ©cution pas Ã  pas
- Points de vigilance / debug

â¡ï¸ **Ce fichier nâ€™est pas destinÃ© au public**

---

## ğŸ§  Choix techniques

- **PostgreSQL (Supabase)** : gratuit, robuste, SQL natif
- **COPY PostgreSQL** : performant sur gros volumes
- **Table brute + vues** : sÃ©paration ingestion / logique mÃ©tier
- **Monitoring SQL versionnÃ©** : explicite et traÃ§able
- **Streamlit** : simple, rapide, dÃ©monstratif

